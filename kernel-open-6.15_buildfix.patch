diff -uNrp a/kernel-open/Kbuild b/kernel-open/Kbuild
--- a/kernel-open/Kbuild	2025-04-10 16:55:42.000000000 +0100
+++ b/kernel-open/Kbuild	2025-04-27 10:43:28.345532688 +0100
@@ -75,21 +75,15 @@ $(foreach _module, $(NV_KERNEL_MODULES),
  $(eval include $(src)/$(_module)/$(_module).Kbuild))
 
 
-#
-# Define CFLAGS that apply to all the NVIDIA kernel modules. EXTRA_CFLAGS
-# is deprecated since 2.6.24 in favor of ccflags-y, but we need to support
-# older kernels which do not have ccflags-y. Newer kernels append
-# $(EXTRA_CFLAGS) to ccflags-y for compatibility.
-#
-
-EXTRA_CFLAGS += -I$(src)/common/inc
-EXTRA_CFLAGS += -I$(src)
-EXTRA_CFLAGS += -Wall $(DEFINES) $(INCLUDES) -Wno-cast-qual -Wno-format-extra-args
-EXTRA_CFLAGS += -D__KERNEL__ -DMODULE -DNVRM
-EXTRA_CFLAGS += -DNV_VERSION_STRING=\"575.51.02\"
+ccflags-y += -std=gnu17
+ccflags-y += -I$(src)/common/inc
+ccflags-y += -I$(src)
+ccflags-y += -Wall $(DEFINES) $(INCLUDES) -Wno-cast-qual -Wno-format-extra-args
+ccflags-y += -D__KERNEL__ -DMODULE -DNVRM
+ccflags-y += -DNV_VERSION_STRING=\"575.51.02\"
 
 ifneq ($(SYSSRCHOST1X),)
- EXTRA_CFLAGS += -I$(SYSSRCHOST1X)
+ ccflags-y += -I$(SYSSRCHOST1X)
 endif
 
 # Some Android kernels prohibit driver use of filesystem functions like
@@ -99,57 +93,57 @@ endif
 PLATFORM_IS_ANDROID ?= 0
 
 ifeq ($(PLATFORM_IS_ANDROID),1)
- EXTRA_CFLAGS += -DNV_FILESYSTEM_ACCESS_AVAILABLE=0
+ ccflags-y += -DNV_FILESYSTEM_ACCESS_AVAILABLE=0
 else
- EXTRA_CFLAGS += -DNV_FILESYSTEM_ACCESS_AVAILABLE=1
+ ccflags-y += -DNV_FILESYSTEM_ACCESS_AVAILABLE=1
 endif
 
-EXTRA_CFLAGS += -Wno-unused-function
+ccflags-y += -Wno-unused-function
 
 ifneq ($(NV_BUILD_TYPE),debug)
- EXTRA_CFLAGS += -Wuninitialized
+ ccflags-y += -Wuninitialized
 endif
 
-EXTRA_CFLAGS += -fno-strict-aliasing
+ccflags-y += -fno-strict-aliasing
 
 ifeq ($(ARCH),arm64)
- EXTRA_CFLAGS += -mstrict-align
+ ccflags-y += -mstrict-align
 endif
 
 ifeq ($(NV_BUILD_TYPE),debug)
- EXTRA_CFLAGS += -g
+ ccflags-y += -g
 endif
 
-EXTRA_CFLAGS += -ffreestanding
+ccflags-y += -ffreestanding
 
 ifeq ($(ARCH),arm64)
- EXTRA_CFLAGS += -mgeneral-regs-only -march=armv8-a
- EXTRA_CFLAGS += $(call cc-option,-mno-outline-atomics,)
+ ccflags-y += -mgeneral-regs-only -march=armv8-a
+ ccflags-y += $(call cc-option,-mno-outline-atomics,)
 endif
 
 ifeq ($(ARCH),x86_64)
- EXTRA_CFLAGS += -mno-red-zone -mcmodel=kernel
+ ccflags-y += -mno-red-zone -mcmodel=kernel
 endif
 
 ifeq ($(ARCH),powerpc)
- EXTRA_CFLAGS += -mlittle-endian -mno-strict-align
+ ccflags-y += -mlittle-endian -mno-strict-align
 endif
 
-EXTRA_CFLAGS += -DNV_UVM_ENABLE
-EXTRA_CFLAGS += $(call cc-option,-Werror=undef,)
-EXTRA_CFLAGS += -DNV_SPECTRE_V2=$(NV_SPECTRE_V2)
-EXTRA_CFLAGS += -DNV_KERNEL_INTERFACE_LAYER
+ccflags-y += -DNV_UVM_ENABLE
+ccflags-y += $(call cc-option,-Werror=undef,)
+ccflags-y += -DNV_SPECTRE_V2=$(NV_SPECTRE_V2)
+ccflags-y += -DNV_KERNEL_INTERFACE_LAYER
 
 #
 # Detect SGI UV systems and apply system-specific optimizations.
 #
 
 ifneq ($(wildcard /proc/sgi_uv),)
- EXTRA_CFLAGS += -DNV_CONFIG_X86_UV
+ ccflags-y += -DNV_CONFIG_X86_UV
 endif
 
 ifdef VGX_FORCE_VFIO_PCI_CORE
- EXTRA_CFLAGS += -DNV_VGPU_FORCE_VFIO_PCI_CORE
+ ccflags-y += -DNV_VGPU_FORCE_VFIO_PCI_CORE
 endif
 
 WARNINGS_AS_ERRORS ?=
@@ -183,7 +177,7 @@ NV_CONFTEST_CMD := /bin/sh $(NV_CONFTEST
 
 NV_CFLAGS_FROM_CONFTEST := $(shell $(NV_CONFTEST_CMD) build_cflags)
 
-NV_CONFTEST_CFLAGS = $(NV_CFLAGS_FROM_CONFTEST) $(EXTRA_CFLAGS) -fno-pie
+NV_CONFTEST_CFLAGS = $(NV_CFLAGS_FROM_CONFTEST) $(ccflags-y) -fno-pie
 NV_CONFTEST_CFLAGS += $(call cc-disable-warning,pointer-sign)
 NV_CONFTEST_CFLAGS += $(call cc-option,-fshort-wchar,)
 NV_CONFTEST_CFLAGS += $(call cc-option,-Werror=incompatible-pointer-types,)
diff -uNrp a/kernel-open/nvidia/nv.c b/kernel-open/nvidia/nv.c
--- a/kernel-open/nvidia/nv.c	2025-04-10 16:55:10.000000000 +0100
+++ b/kernel-open/nvidia/nv.c	2025-04-27 10:22:33.344672970 +0100
@@ -22,6 +22,7 @@
  */
 
 #include <linux/module.h>  // for MODULE_FIRMWARE
+#include <linux/version.h>
 
 // must precede "nv.h" and "nv-firmware.h" includes
 #define NV_FIRMWARE_FOR_NAME(name)  "nvidia/" NV_VERSION_STRING "/" name ".bin"
@@ -4189,7 +4190,11 @@ int NV_API_CALL nv_stop_rc_timer(
 
     nv_printf(NV_DBG_INFO, "NVRM: stopping rc timer\n");
     nv->rc_timer_enabled = 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
     del_timer_sync(&nvl->rc_timer.kernel_timer);
+#else
+    timer_delete_sync(&nvl->rc_timer.kernel_timer);
+#endif
     nv_printf(NV_DBG_INFO, "NVRM: rc timer stopped\n");
 
     return 0;
@@ -4233,7 +4238,11 @@ void NV_API_CALL nv_stop_snapshot_timer(
     NV_SPIN_UNLOCK_IRQRESTORE(&nvl->snapshot_timer_lock, flags);
 
     if (timer_active)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
         del_timer_sync(&nvl->snapshot_timer.kernel_timer);
+#else
+        timer_delete_sync(&nvl->snapshot_timer.kernel_timer);
+#endif
 }
 
 void NV_API_CALL nv_flush_snapshot_timer(void)
diff -uNrp a/kernel-open/nvidia/nv-nano-timer.c b/kernel-open/nvidia/nv-nano-timer.c
--- a/kernel-open/nvidia/nv-nano-timer.c	2025-04-10 16:55:10.000000000 +0100
+++ b/kernel-open/nvidia/nv-nano-timer.c	2025-04-27 10:22:33.344377367 +0100
@@ -27,6 +27,7 @@
 #include <linux/hrtimer.h>
 #include <linux/ktime.h>
 #include <linux/timer.h>
+#include <linux/version.h>
 #include "os-interface.h"
 #include "nv-linux.h"
 
@@ -46,7 +47,7 @@ struct nv_nano_timer
 };
 
 /*!
- * @brief runs nano second resolution timer callback 
+ * @brief runs nano second resolution timer callback
 *
  * @param[in] nv_nstimer    Pointer to nv_nano_timer_t object
  */
@@ -150,9 +151,14 @@ void NV_API_CALL nv_create_nano_timer(
     nv_nstimer->nv_nano_timer_callback = nvidia_nano_timer_callback;
 
 #if NV_NANO_TIMER_USE_HRTIMER
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
     hrtimer_init(&nv_nstimer->hr_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
     nv_nstimer->hr_timer.function = nv_nano_timer_callback_typed_data;
 #else
+    hrtimer_setup(&nv_nstimer->hr_timer, nv_nano_timer_callback_typed_data,
+                  CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+#endif /* Kernel < 6.15 */
+#else
 #if defined(NV_TIMER_SETUP_PRESENT)
     timer_setup(&nv_nstimer->jiffy_timer, nv_jiffy_timer_callback_typed_data, 0);
 #else
@@ -203,7 +209,11 @@ void NV_API_CALL nv_cancel_nano_timer(
 #if NV_NANO_TIMER_USE_HRTIMER
     hrtimer_cancel(&nv_nstimer->hr_timer);
 #else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
     del_timer_sync(&nv_nstimer->jiffy_timer);
+#else
+    timer_delete_sync(&nv_nstimer->jiffy_timer);
+#endif
 #endif
 
 }
diff -uNrp a/kernel-open/nvidia-drm/nvidia-drm-os-interface.c b/kernel-open/nvidia-drm/nvidia-drm-os-interface.c
--- a/kernel-open/nvidia-drm/nvidia-drm-os-interface.c	2025-04-10 16:41:35.000000000 +0100
+++ b/kernel-open/nvidia-drm/nvidia-drm-os-interface.c	2025-04-27 10:22:33.344075442 +0100
@@ -21,6 +21,7 @@
  */
 
 #include <linux/slab.h>
+#include <linux/version.h>
 
 #include "nvidia-drm-os-interface.h"
 
@@ -238,7 +239,11 @@ unsigned long nv_drm_timeout_from_ms(NvU
 
 bool nv_drm_del_timer_sync(nv_drm_timer *timer)
 {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
     if (del_timer_sync(&timer->kernel_timer)) {
+#else
+    if (timer_delete_sync(&timer->kernel_timer)) {
+#endif
         return true;
     } else {
         return false;
diff -uNrp a/kernel-open/nvidia-modeset/nvidia-modeset-linux.c b/kernel-open/nvidia-modeset/nvidia-modeset-linux.c
--- a/kernel-open/nvidia-modeset/nvidia-modeset-linux.c	2025-04-10 16:40:55.000000000 +0100
+++ b/kernel-open/nvidia-modeset/nvidia-modeset-linux.c	2025-04-27 10:22:33.344225618 +0100
@@ -37,6 +37,7 @@
 #include <linux/freezer.h>
 #include <linux/poll.h>
 #include <linux/cdev.h>
+#include <linux/version.h>
 
 #include <acpi/video.h>
 
@@ -748,7 +749,11 @@ static void nvkms_kthread_q_callback(voi
      * pending timers and than waiting for workqueue callbacks.
      */
     if (timer->kernel_timer_created) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
         del_timer_sync(&timer->kernel_timer);
+#else
+        timer_delete_sync(&timer->kernel_timer);
+#endif
     }
 
     /*
@@ -1932,7 +1937,11 @@ restart:
              * completion, and we wait for queue completion with
              * nv_kthread_q_stop below.
              */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
             if (del_timer_sync(&timer->kernel_timer) == 1) {
+#else
+            if (timer_delete_sync(&timer->kernel_timer) == 1) {
+#endif
                 /*  We've deactivated timer so we need to clean after it */
                 list_del(&timer->timers_list);
 
diff -uNrp a/kernel-open/nvidia-uvm/uvm_ats_sva.c b/kernel-open/nvidia-uvm/uvm_ats_sva.c
--- a/kernel-open/nvidia-uvm/uvm_ats_sva.c	2025-04-10 18:08:50.000000000 +0100
+++ b/kernel-open/nvidia-uvm/uvm_ats_sva.c	2025-04-27 10:24:47.591844968 +0100
@@ -20,6 +20,7 @@
     DEALINGS IN THE SOFTWARE.
 
 *******************************************************************************/
+#include <linux/version.h>
 
 #include "uvm_ats_sva.h"
 
@@ -137,10 +138,17 @@ static NvU32 smmu_vcmdq_read32(void __io
     return ioread32(SMMU_VCMDQ_BASE_ADDR(smmu_cmdqv_base, VCMDQ) + reg);
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
 static void smmu_vcmdq_write64(void __iomem *smmu_cmdqv_base, int reg, NvU64 val)
 {
     iowrite64(val, SMMU_VCMDQ_BASE_ADDR(smmu_cmdqv_base, VCMDQ) + reg);
 }
+#else
+static void smmu_vcmdq_write64(void __iomem *smmu_cmdqv_base, int reg, NvU64 val)
+{
+    __iowrite64_hi_lo(val, SMMU_VCMDQ_BASE_ADDR(smmu_cmdqv_base, VCMDQ) + reg);
+}
+#endif
 
 // Fix for Bug 4130089: [GH180][r535] WAR for kernel not issuing SMMU
 // TLB invalidates on read-only to read-write upgrades
diff -uNrp a/kernel-open/nvidia-uvm/uvm_common.h b/kernel-open/nvidia-uvm/uvm_common.h
--- a/kernel-open/nvidia-uvm/uvm_common.h	2025-04-10 16:55:21.000000000 +0100
+++ b/kernel-open/nvidia-uvm/uvm_common.h	2025-04-27 10:26:11.083106295 +0100
@@ -39,6 +39,12 @@
     #define UVM_IS_DEVELOP() 0
 #endif
 
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
+#define page_pgmap(p) (p)->pgmap
+#endif
+
 #include "uvm_types.h"
 #include "uvm_linux.h"
 
diff -uNrp a/kernel-open/nvidia-uvm/uvm_hmm.c b/kernel-open/nvidia-uvm/uvm_hmm.c
--- a/kernel-open/nvidia-uvm/uvm_hmm.c	2025-04-10 16:55:36.000000000 +0100
+++ b/kernel-open/nvidia-uvm/uvm_hmm.c	2025-04-27 10:35:38.316865361 +0100
@@ -1992,7 +1992,7 @@ static void fill_dst_pfn(uvm_va_block_t
 
     dpage = pfn_to_page(pfn);
     UVM_ASSERT(is_device_private_page(dpage));
-    UVM_ASSERT(dpage->pgmap->owner == &g_uvm_global);
+    UVM_ASSERT(page_pgmap(dpage)->owner == &g_uvm_global);
 
     hmm_mark_gpu_chunk_referenced(va_block, gpu, gpu_chunk);
     UVM_ASSERT(!page_count(dpage));
@@ -2438,6 +2438,7 @@ static void hmm_release_atomic_pages(uvm
     }
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 15, 0)
 static NV_STATUS hmm_block_atomic_fault_locked(uvm_processor_id_t processor_id,
                                                uvm_va_block_t *va_block,
                                                uvm_va_block_retry_t *va_block_retry,
@@ -2556,6 +2557,135 @@ release:
 done:
     return status;
 }
+#else
+static NV_STATUS hmm_block_atomic_fault_locked(uvm_processor_id_t processor_id,
+                                               uvm_va_block_t *va_block,
+                                               uvm_va_block_retry_t *va_block_retry,
+                                               uvm_service_block_context_t *service_context)
+{
+    uvm_va_block_region_t region = service_context->region;
+    struct page **pages = service_context->block_context->hmm.pages;
+    struct vm_area_struct *vma = service_context->block_context->hmm.vma;
+    struct page *page;
+    struct folio *folio;
+    uvm_page_index_t page_index;
+    uvm_make_resident_cause_t cause;
+    NV_STATUS status;
+
+    if (!uvm_processor_mask_test(&va_block->resident, UVM_ID_CPU) ||
+        !uvm_va_block_cpu_is_region_resident_on(va_block, NUMA_NO_NODE, region)) {
+        // There is an atomic GPU fault. We need to make sure no pages are
+        // GPU resident so that make_device_exclusive_range() doesn't call
+        // migrate_to_ram() and cause a va_space lock recursion problem.
+        if (service_context->operation == UVM_SERVICE_OPERATION_REPLAYABLE_FAULTS)
+            cause = UVM_MAKE_RESIDENT_CAUSE_REPLAYABLE_FAULT;
+        else if (service_context->operation == UVM_SERVICE_OPERATION_NON_REPLAYABLE_FAULTS)
+            cause = UVM_MAKE_RESIDENT_CAUSE_NON_REPLAYABLE_FAULT;
+        else
+            cause = UVM_MAKE_RESIDENT_CAUSE_ACCESS_COUNTER;
+
+        UVM_ASSERT(uvm_hmm_check_context_vma_is_valid(va_block, vma, region));
+
+        status = uvm_hmm_va_block_migrate_locked(va_block, va_block_retry, service_context, UVM_ID_CPU, region, cause);
+        if (status != NV_OK)
+            goto done;
+
+        // make_device_exclusive_range() will try to call migrate_to_ram()
+        // and deadlock with ourself if the data isn't CPU resident.
+        if (!uvm_processor_mask_test(&va_block->resident, UVM_ID_CPU) ||
+            !uvm_va_block_cpu_is_region_resident_on(va_block, NUMA_NO_NODE, region)) {
+            status = NV_WARN_MORE_PROCESSING_REQUIRED;
+            goto done;
+        }
+    }
+
+    // TODO: Bug 4014681: atomic GPU operations are not supported on MAP_SHARED
+    // mmap() files so we check for that here and report a fatal fault.
+    // Otherwise with the current Linux 6.1 make_device_exclusive_range(),
+    // it doesn't make the page exclusive and we end up in an endless loop.
+    if (vma->vm_flags & (VM_SHARED | VM_HUGETLB)) {
+        status = NV_ERR_NOT_SUPPORTED;
+        goto done;
+    }
+
+    hmm_range_fault_begin(va_block);
+
+    uvm_mutex_unlock(&va_block->lock);
+
+    unsigned long start = uvm_va_block_cpu_page_address(va_block, region.first);
+    page = make_device_exclusive(service_context->block_context->mm, start,
+            &g_uvm_global, &folio);
+
+    uvm_mutex_lock(&va_block->lock);
+
+    if (IS_ERR(page)) {
+        long err = PTR_ERR(page);
+        status = (err == -EBUSY) ? NV_WARN_MORE_PROCESSING_REQUIRED : errno_to_nv_status(err);
+        goto done;
+    }
+
+    /*
+     * This code is most likely WRONG, but it *should* be relatively safe
+     * because of the error above
+     */
+    size_t npages = (uvm_va_block_cpu_page_address(va_block, region.outer - 1) + PAGE_SIZE -
+            start) >> PAGE_SHIFT;
+    while (npages < uvm_va_block_region_num_pages(region))
+        pages[region.first + npages++] = NULL;
+
+    folio_unlock(folio);
+    folio_put(folio);
+
+    if (hmm_range_fault_retry(va_block)) {
+        status = NV_WARN_MORE_PROCESSING_REQUIRED;
+        goto release;
+    }
+
+    status = NV_OK;
+
+    for_each_va_block_page_in_region(page_index, region) {
+        struct page *page = pages[page_index];
+
+        if (!page) {
+            // Record that one of the pages isn't exclusive but keep converting
+            // the others.
+            status = NV_WARN_MORE_PROCESSING_REQUIRED;
+            continue;
+        }
+
+        // If a CPU chunk is already allocated, check to see it matches what
+        // make_device_exclusive_range() found.
+        if (uvm_page_mask_test(&va_block->cpu.allocated, page_index)) {
+            UVM_ASSERT(hmm_va_block_cpu_page_is_same(va_block, page_index, page));
+            UVM_ASSERT(uvm_processor_mask_test(&va_block->resident, UVM_ID_CPU));
+            UVM_ASSERT(uvm_va_block_cpu_is_page_resident_on(va_block, NUMA_NO_NODE, page_index));
+        }
+        else {
+            NV_STATUS s = hmm_va_block_cpu_page_populate(va_block, page_index, page);
+
+            if (s == NV_OK)
+                uvm_va_block_cpu_set_resident_page(va_block, page_to_nid(page), page_index);
+        }
+
+        cpu_mapping_clear(va_block, page_index);
+    }
+
+    if (status != NV_OK)
+        goto release;
+
+    status = uvm_va_block_service_copy(processor_id, UVM_ID_CPU, va_block, va_block_retry, service_context);
+    if (status != NV_OK)
+        goto release;
+
+    status = uvm_va_block_service_finish(processor_id, va_block, service_context);
+
+release:
+    hmm_release_atomic_pages(va_block, service_context);
+
+done:
+    return status;
+}
+#endif
 
 static bool is_atomic_fault(NvU8 *access_type, uvm_va_block_region_t region)
 {
diff -uNrp a/kernel-open/nvidia-uvm/uvm_pmm_gpu.c b/kernel-open/nvidia-uvm/uvm_pmm_gpu.c
--- a/kernel-open/nvidia-uvm/uvm_pmm_gpu.c	2025-04-10 16:55:35.000000000 +0100
+++ b/kernel-open/nvidia-uvm/uvm_pmm_gpu.c	2025-04-27 10:26:11.083709855 +0100
@@ -3333,7 +3333,7 @@ void uvm_pmm_gpu_device_p2p_init(uvm_gpu
     // TODO: Bug 4672502: [Linux Upstream][UVM] Allow drivers to manage and
     // allocate PCI P2PDMA pages directly
     p2p_page = pfn_to_page(pci_start_pfn);
-    p2p_page->pgmap->ops = &uvm_device_p2p_pgmap_ops;
+    page_pgmap(p2p_page)->ops = &uvm_device_p2p_pgmap_ops;
     for (; page_to_pfn(p2p_page) < pci_end_pfn; p2p_page++)
         p2p_page->zone_device_data = NULL;
 
@@ -3348,7 +3348,7 @@ void uvm_pmm_gpu_device_p2p_deinit(uvm_g
 
     if (gpu->device_p2p_initialised && !uvm_parent_gpu_is_coherent(gpu->parent)) {
         p2p_page = pfn_to_page(pci_start_pfn);
-        devm_memunmap_pages(&gpu->parent->pci_dev->dev, p2p_page->pgmap);
+        devm_memunmap_pages(&gpu->parent->pci_dev->dev, page_pgmap(p2p_page));
     }
 
     gpu->device_p2p_initialised = false;
